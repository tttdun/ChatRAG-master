{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-09-30T07:36:24.006433Z",
     "start_time": "2024-09-30T07:36:20.004645Z"
    }
   },
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from typing import List, cast\n",
    "from pdf2image import convert_from_path\n",
    "from PIL import Image\n",
    "import torch\n",
    "from torch import nn\n",
    "from transformers import LlamaTokenizerFast, PaliGemmaProcessor\n",
    "from transformers.models.paligemma.configuration_paligemma import PaliGemmaConfig\n",
    "from transformers.models.paligemma.modeling_paligemma import PaliGemmaForConditionalGeneration, PaliGemmaPreTrainedModel\n",
    "import os"
   ],
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Define ColPali model\n",
    "\n",
    "Chức năng chính của lớp này là tạo ra các biểu diễn đầu ra (embeddings) đã được chiếu (projected embeddings) và chuẩn hoá."
   ],
   "id": "93c57a7f60518908"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-30T07:36:24.015424Z",
     "start_time": "2024-09-30T07:36:24.010501Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class ColPali(PaliGemmaPreTrainedModel):\n",
    "    def __init__(self, config: PaliGemmaConfig):\n",
    "        super(ColPali, self).__init__(config=config)\n",
    "        self.model = PaliGemmaForConditionalGeneration(config)\n",
    "        self.dim = 128\n",
    "        self.custom_text_proj = nn.Linear(self.model.config.text_config.hidden_size, self.dim)\n",
    "        self.main_input_name = \"doc_input_ids\"\n",
    "\n",
    "    def forward(self, *args, **kwargs) -> torch.Tensor:\n",
    "        outputs = self.model(*args, output_hidden_states=True, **kwargs)\n",
    "        last_hidden_states = outputs.hidden_states[-1]\n",
    "        proj = self.custom_text_proj(last_hidden_states)\n",
    "        proj = proj / proj.norm(dim=-1, keepdim=True)\n",
    "        '''\n",
    "        attention_mask ban đầu có dạng [batch_size, sequence_length], tức là một ma trận 2 chiều. \n",
    "        Mỗi giá trị trong attention_mask tương ứng với một token trong chuỗi đầu vào.\n",
    "        Phép .unsqueeze(-1) thêm một chiều mới ở vị trí cuối cùng, biến đổi attention_mask từ [batch_size, sequence_length] thành [batch_size, sequence_length, 1]. \n",
    "        Điều này giúp cho phép nhân với tensor proj (kích thước [batch_size, sequence_length, self.dim]) có thể thực hiện được.\n",
    "        \n",
    "        Khi đó, phép nhân giữa proj và attention_mask.unsqueeze(-1) sẽ chỉ giữ lại các embedding của các token thực (với mask là 1) \n",
    "        và làm bằng 0 các embedding của token padding (với mask là 0).\n",
    "        '''\n",
    "        proj = proj * kwargs[\"attention_mask\"].unsqueeze(-1)\n",
    "        return proj"
   ],
   "id": "61d0cc59a8b5df6f",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Define input classes\n",
    "\n",
    "- ColPaliTextInput: Được dùng để chứa dữ liệu đầu vào dạng văn bản (text), với các thành phần chính là input_ids (mã token) và attention_mask (mặt nạ).\n",
    "\n",
    "- ColPaliImageInput: Được dùng để chứa dữ liệu đầu vào là văn bản kèm với hình ảnh. Ngoài input_ids và attention_mask, lớp này còn chứa một tensor pixel_values để lưu thông tin pixel của ảnh.\n",
    "\n",
    "- Cả hai lớp đều có phương thức to(device), cho phép dễ dàng di chuyển dữ liệu sang các thiết bị khác nhau, như GPU hoặc CPU, giúp tối ưu hóa quá trình xử lý."
   ],
   "id": "3f2e16893fa1547d"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-30T07:36:24.026515Z",
     "start_time": "2024-09-30T07:36:24.022030Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class ColPaliTextInput:\n",
    "    def __init__(self, input_ids, attention_mask):\n",
    "        self.input_ids = input_ids\n",
    "        self.attention_mask = attention_mask\n",
    "\n",
    "    def to(self, device):\n",
    "        return ColPaliTextInput(\n",
    "            input_ids=self.input_ids.to(device),\n",
    "            attention_mask=self.attention_mask.to(device),\n",
    "        )\n",
    "\n",
    "class ColPaliImageInput:\n",
    "    def __init__(self, input_ids, pixel_values, attention_mask):\n",
    "        self.input_ids = input_ids\n",
    "        self.pixel_values = pixel_values\n",
    "        self.attention_mask = attention_mask\n",
    "\n",
    "    def to(self, device):\n",
    "        return ColPaliImageInput(\n",
    "            input_ids=self.input_ids.to(device),\n",
    "            pixel_values=self.pixel_values.to(device),\n",
    "            attention_mask=self.attention_mask.to(device),\n",
    "        )"
   ],
   "id": "8badf5f68df323fa",
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Define ColPali Processor",
   "id": "73aa0e9b49c49cd0"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-30T07:36:24.046714Z",
     "start_time": "2024-09-30T07:36:24.037661Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class ColPaliProcessor:\n",
    "    def __init__(self, processor: PaliGemmaProcessor):\n",
    "        self.processor = processor\n",
    "        self.tokenizer = cast(LlamaTokenizerFast, self.processor.tokenizer)\n",
    "\n",
    "    @staticmethod\n",
    "    def from_pretrained(model_name: str) -> 'ColPaliProcessor':\n",
    "        return ColPaliProcessor(processor=PaliGemmaProcessor.from_pretrained(model_name))\n",
    "\n",
    "    def process_text(self, text: str | List[str], padding: str = \"longest\", return_tensors: str = \"pt\", add_special_tokens: bool = True) -> ColPaliTextInput:\n",
    "        if add_special_tokens:\n",
    "            if isinstance(text, str):\n",
    "                text = self.tokenizer.bos_token + text + \"\\n\"\n",
    "            elif isinstance(text, list):\n",
    "                text = [self.tokenizer.bos_token + t + \"\\n\" for t in text]\n",
    "            else:\n",
    "                raise ValueError(\"text must be a string or a list of strings.\")\n",
    "\n",
    "        batch_output = self.tokenizer(text, padding=padding, return_tensors=return_tensors, add_special_tokens=add_special_tokens)\n",
    "\n",
    "        return ColPaliTextInput(\n",
    "            input_ids=batch_output[\"input_ids\"],\n",
    "            attention_mask=batch_output[\"attention_mask\"],\n",
    "        )\n",
    "\n",
    "    def process_image(self, image: Image.Image | List[Image.Image], padding: str = \"longest\", do_convert_rgb: bool = True, return_tensors: str = \"pt\", add_special_prompt: bool = True) -> ColPaliImageInput:\n",
    "        special_prompt = \"Describe the image.\" if add_special_prompt else None\n",
    "        if isinstance(image, Image.Image):\n",
    "            text_input = [special_prompt]\n",
    "        elif isinstance(image, list):\n",
    "            text_input = [special_prompt] * len(image)\n",
    "        else:\n",
    "            raise ValueError(\"image must be a PIL Image or a list of PIL Images.\")\n",
    "\n",
    "        batch_output = self.processor(\n",
    "            text=text_input,\n",
    "            images=image,\n",
    "            padding=padding,\n",
    "            do_convert_rgb=do_convert_rgb,\n",
    "            return_tensors=return_tensors,\n",
    "        )\n",
    "\n",
    "        return ColPaliImageInput(\n",
    "            input_ids=batch_output[\"input_ids\"],\n",
    "            pixel_values=batch_output[\"pixel_values\"],\n",
    "            attention_mask=batch_output[\"attention_mask\"],\n",
    "        )\n",
    "\n",
    "    def decode(self, *args, **kwargs):\n",
    "        return self.tokenizer.decode(*args, **kwargs)\n",
    "\n",
    "    def batch_decode(self, *args, **kwargs):\n",
    "        return self.tokenizer.batch_decode(*args, **kwargs)"
   ],
   "id": "76f44e1b8868fa49",
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Helper functions\n",
    "- convert_pdf_to_images: Chuyển đổi từng tệp PDF thành hình ảnh và lưu trữ chúng.\n",
    "- process_pdfs_with_colpali: Xử lý từng tệp PDF bằng mô hình, lấy embedding của từng trang, và lưu kết quả vào các tệp .npy"
   ],
   "id": "6911e555f4a366bf"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-30T07:36:24.074002Z",
     "start_time": "2024-09-30T07:36:24.068094Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def convert_pdf_to_images(pdf_file: str, save_folder: str) -> List[Image.Image]:\n",
    "    images = convert_from_path(pdf_file)\n",
    "    os.makedirs(save_folder, exist_ok=True)\n",
    "    saved_images = []\n",
    "    for i, image in enumerate(images):\n",
    "        image_path = os.path.join(save_folder, f\"page_{i+1}.jpg\")\n",
    "        image.save(image_path, \"JPEG\")\n",
    "        saved_images.append(Image.open(image_path))\n",
    "    return saved_images\n",
    "\n",
    "def process_pdfs_with_colpali(pdf_files, output_dir, model, processor):\n",
    "    all_embeddings = []\n",
    "    all_page_info = []\n",
    "\n",
    "    for pdf_file in pdf_files:\n",
    "        pdf_images = convert_pdf_to_images(pdf_file, os.path.join(output_dir, \"pdf_images\"))\n",
    "\n",
    "        for page_num, image in enumerate(pdf_images):\n",
    "            image_input = processor.process_image(image).to(model.device)\n",
    "            with torch.no_grad():\n",
    "                page_embedding = model(**vars(image_input))\n",
    "\n",
    "            all_embeddings.append(page_embedding.cpu().numpy().squeeze()) # the last .squeeze() call removes the batch dimension\n",
    "            all_page_info.append({\"pdf\": pdf_file, \"page\": page_num})\n",
    "\n",
    "    embeddings_array = np.array(all_embeddings)\n",
    "\n",
    "    np.save(Path(output_dir) / \"embeddings.npy\", embeddings_array)\n",
    "    np.save(Path(output_dir) / \"page_info.npy\", all_page_info)\n",
    "\n",
    "    return embeddings_array, all_page_info\n"
   ],
   "id": "26353ede306b9b85",
   "outputs": [],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-30T07:36:24.090495Z",
     "start_time": "2024-09-30T07:36:24.085217Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def answer_query_with_colpali(query, embeddings_array, page_info, model, processor):\n",
    "    query_input = processor.process_text(query).to(model.device)\n",
    "    with torch.no_grad():\n",
    "        query_embedding = model(**vars(query_input))\n",
    "\n",
    "    # Reshape embeddings if necessary\n",
    "    if len(embeddings_array.shape) == 3:\n",
    "        embeddings_array = embeddings_array.mean(axis=1)  # Average over sequence dimension\n",
    "    if len(query_embedding.shape) == 3:\n",
    "        query_embedding = query_embedding.mean(axis=1)  # Average over sequence dimension\n",
    "\n",
    "    # Ensure both embeddings are 2D\n",
    "    embeddings_array = embeddings_array.squeeze()\n",
    "    query_embedding = query_embedding.cpu().numpy().squeeze() # remove batch_dimension with the last squeeze call\n",
    "\n",
    "    # Compute similarity scores\n",
    "    similarity_scores = np.dot(embeddings_array, query_embedding.T)\n",
    "\n",
    "    K = 5\n",
    "    top_k_indices = np.argsort(similarity_scores.flatten())[-K:][::-1]\n",
    "\n",
    "    top_results = [\n",
    "        {\"score\": similarity_scores.flatten()[i], \"info\": page_info[i]}\n",
    "        for i in top_k_indices\n",
    "    ]\n",
    "\n",
    "    return top_results"
   ],
   "id": "762bc7ea88afe655",
   "outputs": [],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-30T07:36:32.291915Z",
     "start_time": "2024-09-30T07:36:24.103336Z"
    }
   },
   "cell_type": "code",
   "source": [
    "device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "model_path = \"google/paligemma-3b-mix-448\" # can also try smaller models\n",
    "lora_path = \"vidore/colpali\"\n",
    "\n",
    "model = ColPali.from_pretrained(model_path) # torch_dtype = torch.bfloat16 # there is also a `bitsandbytes` setup for 8-bit / 4-bit\n",
    "model.load_adapter(lora_path, adapter_name=\"colpali\")\n",
    "model.to(device);"
   ],
   "id": "88905347037814ae",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`config.hidden_act` is ignored, you should use `config.hidden_activation` instead.\n",
      "Gemma's activation function will be set to `gelu_pytorch_tanh`. Please, use\n",
      "`config.hidden_activation` if you want to override this behaviour.\n",
      "See https://github.com/huggingface/transformers/pull/29402 for more details.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "cca55d45a41944c2a94a4cc3d4f5f432"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of ColPali were not initialized from the model checkpoint at google/paligemma-3b-mix-448 and are newly initialized: ['custom_text_proj.bias', 'custom_text_proj.weight', 'language_model.lm_head.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "The installed version of bitsandbytes was compiled without GPU support. 8-bit optimizers, 8-bit multiplication, and GPU quantization are unavailable.\n"
     ]
    }
   ],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-30T07:43:49.913948Z",
     "start_time": "2024-09-30T07:36:32.391537Z"
    }
   },
   "cell_type": "code",
   "source": [
    "processor = ColPaliProcessor.from_pretrained(model_path)  # Load processor\n",
    "\n",
    "# Danh sách các đường dẫn tới tệp PDF trên máy của bạn\n",
    "pdf_files = [\n",
    "    r\"C:\\Users\\DuyTVB\\PycharmProjects\\ChatRAG\\doc\\ChartReportUTE.pdf\" # Thay thế bằng đường dẫn thực tế của tệp trên máy bạn\n",
    "]\n",
    "# Output directory to save embeddings\n",
    "output_dir = \"colpali_output\"\n",
    "# Kiểm tra xem thư mục output có tồn tại không, nếu không thì tạo mới\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "# Process the PDF files and save embeddings\n",
    "embeddings, page_info = process_pdfs_with_colpali(pdf_files, output_dir, model, processor)"
   ],
   "id": "39c63d708a7095aa",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Starting from v4.46, the `logits` model output will have the same type as the model (except at train time, where it will always be FP32)\n"
     ]
    }
   ],
   "execution_count": 9
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-30T07:43:56.339882Z",
     "start_time": "2024-09-30T07:43:50.304666Z"
    }
   },
   "cell_type": "code",
   "source": [
    "query = \"Sơ đồ về tổng thí sinh nhập học năm 2019?\" # The answer should be contained in NBA-mvp-voting.pdf\n",
    "results = answer_query_with_colpali(query, embeddings, page_info, model, processor)\n",
    "\n",
    "# Print results\n",
    "for result in results:\n",
    "  print(f\"Score: {result['score']}, PDF: {result['info']['pdf']}, Page: {result['info']['page']}\")\n"
   ],
   "id": "a00ddd3726a2e459",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score: 0.4399455189704895, PDF: C:\\Users\\DuyTVB\\PycharmProjects\\ChatRAG\\doc\\ChartReportUTE.pdf, Page: 0\n",
      "Score: 0.4128127098083496, PDF: C:\\Users\\DuyTVB\\PycharmProjects\\ChatRAG\\doc\\ChartReportUTE.pdf, Page: 1\n",
      "Score: 0.37878865003585815, PDF: C:\\Users\\DuyTVB\\PycharmProjects\\ChatRAG\\doc\\ChartReportUTE.pdf, Page: 2\n",
      "Score: 0.34476253390312195, PDF: C:\\Users\\DuyTVB\\PycharmProjects\\ChatRAG\\doc\\ChartReportUTE.pdf, Page: 3\n"
     ]
    }
   ],
   "execution_count": 10
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
